%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document


\usepackage{subfigure}
\usepackage{graphicx}
% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{listings}
\usepackage{booktabs}
\usepackage{cite}
\usepackage{float}
%\usepackage{hyperref}
\usepackage{footmisc}
\usepackage[vlined,boxed,commentsnumbered,ruled]{algorithm2e}
\usepackage{array}
\usepackage{algorithmic}

\title{\LARGE \bf
Monocular Visual Localization using Road Structural Features
}

\author{Yufeng Yu$^{*}$, Huijing Zhao$^{*}$, Franck Davoine$^{+}$, Jinshi Cui$^{*}$, Hongbin Zha$^{*}$
\thanks{$^{*}$Y. Yu, H. Zhao, J. Cui and H. Zha are with the State Key Lab of Machine Perception (MOE), Peking University, Beijing, China. {\tt\small yuyufeng at pku.edu.cn}}
\thanks{$^{+}$F. Davoine is with CNRS, LIAMA Sino-French Laboratory, Beijing, China.}%
}


\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}
Precise localization is an essential issue for autonomous driving applications, where GPS-based systems are challenged to meet requirements such as lane-level accuracy. This paper introduces a new visual-based localization approach in dynamic traffic environments, focusing on and exploiting properties of structured roads like straight roads or intersections. Such environments show several line segments on lane markings, curbs, poles, building edges, etc., which demonstrate the road's longitude, latitude and vertical directions. Based on this observation, we define a Road Structural Feature (RSF) as sets of segments along three perpendicular axes together with some feature points. At each video frame, the proper road structure (or multiple road structures in case of a road intersection) is predicted based on the geometric information given by a 2D map. The RSF is then detected from line segments and points extracted from the image, and used to estimate the pose of the vehicle. Experiments are conducted using video streams collected on major roads in downtown Beijing, which are structured and with intense dynamic traffic. GPS/IMU data have been collected and synchronized with the video streams as a reference in validation. The results show good performance compared with that of a more traditional visual odometry method. Future work will be addressed on using visual approach to improve GPS localization accuracy.
\end{abstract}


\section{Introduction}
\label{sec_introduction}

For decades, there have been lots of research efforts in developing autonomous/assisted driving and mobile mapping systems \cite{urmson2008autonomous}. An essential issue of these systems is the precision of the localization of a vehicle platform in a real world traffic environment, where approaches using different sensors have been studied extensively.

GPS (Global Positioning System) are widely used in car navigation systems to visualize the car's location on a map, or to assist human drivers' decision making.
However, for the applications such as autonomous driving, GPS-based localizations have many challenges.
GPS signals could be obstructed by bridges, tunnels, buildings, etc., and its accuracy could be greatly degraded due to the multi-path problem especially in a downtown area. Although an IMU (Inertial Measure Unit) can be used to interpolate the position during the period of GPS outage \cite{caron2006gps}, its accuracy is maintained only for short periods due to the accelerometer biases and gyro drifts.
More importantly, an autonomous vehicle needs to find its location on road lanes, e.g. if the vehicle will make a right turn, it needs to be or change to the rightmost lane, etc., while the state-of-the-art GPS-based localization systems can not provide at lane-level accuracy. It is thus of great demand on developing localization approaches using on-board high dimensional sensors, such as a LiDAR (Light Detection and Ranging), a mono camera or a stereo vision system.

This research studies a visual-based approach to achieve precise localization in structured road environments using a monocular camera, where high dynamic traffic is one of the greatest challenges. In a structured road environment, the line features on lane markings, curbs, poles, building edges, etc. are dominant, and demonstrate road's longitude, latitude and vertical directions.
A new feature, named road structural feature (RSF), is composed of a set of line segments, which characterize the structured road environment using the three perpendicular axes at a local road frame, together with feature points.
A RSF-based visual localization method using a monocular camera is proposed, where a prior knowledge of the environment, such as a rough map with skeleton road structure, is implemented. After predicting the road structure of the scene (or multiple road structures at intersections), the vehicle pose is estimated after detecting the RSF from a monocular image.
Experiments have been conducted using video streams that were collected on major roads in downtown Beijing, which are structured and dynamic. In this particular work, GPS/IMU data were collected and synchronized with the video streams as a reference in validation, while future work will be addressed on a fusion-based system using visual approach to improve GPS localization accuracy.

The paper is organized as follows. A review to the state-of-the-art localization approaches using on-board high dimensional sensors as well as an analysis of the major challenges is given in section \ref{sec_background}. An overview of the system is presented in section  \ref{sec_overview}. The details of RSF-based localization are addressed in section \ref{sec_details}. Experimental results are demonstrated and discussed in section \ref{sec_experiments}, followed by conclusion and future works in Section  \ref{sec_conclusion}.

\section{Background}
\label{sec_background}

The use of high dimensional sensors, such as a LiDAR (Light Detection and Ranging), a mono camera or a stereo vision system, has been studied to find vehicle pose, the problem of which falls into the realm of SLAM (Simultaneous Localization And Mapping) \cite{durrant2006simultaneous}. LiDAR is an active sensor measuring range distances by estimating the TOF (Time-Of-Flight) of laser beams to surrounding objects. A 2D LiDAR scanning horizontal planes is well used to localize a robot on a flat area \cite{zhang2000line}. 3D LiDAR has also been studied for 6DOF localization \cite{moosmann2011velodyne}, while the high cost or long scanning period put many restrictions on its real applications. Estimating a platform's motion using visual input is also known as visual odometry or visual localization \cite{nister2004visual}. Due to the low cost, high frequency and diversity of camera sensors, visual odometry has been applied on many different kinds of systems.
Estimating robot motion using visual odometry can be traced back to \cite{moravec1980obstacle}. Good results are found in \cite{nister2004visual}. After that, many researchers are doing improvements based on basic visual odometry algorithm. Mouragnon \emph{et al}.\cite{mouragnon2006real} introduced a fast local bundle adjustment method, which made the result more accurate and consistent. Parra \emph{et al}.\cite{parra2011visual} fused a stereo-vision based visual odometry with map data for GPS navigation assistance, which kept track of the vehicle position on a road of a digital map. Ramalingam \emph{et al}.\cite{ramalingam2011pose, ramalingam2010skyline2gps} used the 3D model of the city to get the correspondence between the 2D image space and 3D world space. The results were excellent, but an accurate city model was needed.

%Estimating robot motion using visual odometry can be traced back to [Moravec 1980]. Good results are found in [select some representative works, e.g. Matthies and Shafer 1987, Kriegman, Triendl and Binford 1989, Zhang, Faugeras and Ayache 1988, Lacroix et al. 1999, Mallet, Lacroix and Gallo 2000, Jung and Lacroix 2005, Maimone et al. 2004, Nister 2006). However few of the existing approaches address the challenges in a dynamic traffic environment, such as an urban street.

\begin{figure}
\centering
 \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{source//Final//image0130_277in785.png}
    \label{fig:line_feature}
  \end{minipage}%
  \hfill
 \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{source//Final//image0130_84in144.png}
    \label{fig:point_feature}
  \end{minipage}%
 \vfill
 \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{source//Final//image0130_277in785_blank.png}
    \label{fig:line_in_blank}
  \end{minipage}%
  \hfill
 \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{source//Final//image0130_84in144_blank.png}
    \label{fig:point_int_blank}
  \end{minipage}%
  \vfill
 \begin{minipage}[t]{0.48\linewidth}
    \centering
    (a)
  \end{minipage}%
  \hfill
 \begin{minipage}[t]{0.48\linewidth}
    \centering
    (b)
  \end{minipage}%
\caption{Different features detected from a certain frame. Pictures in column (a) show the point features detected using optical flow. The points in red or green are detected on the moving or static objects. Pictures in column (b) show the line features detected using LSD (Line Segment Detector) \cite{von2012lsd}. The line segments in blue, red and green demonstrate the road's longitude, latitude and vertical directions.}
\label{fig:features}
\end{figure}

The critical point of a visual odometry system is a feature tracker, where feature extraction and correlation are essential for vehicle pose estimation. Many methods detect and match the feature points of successive frames for localization. And in order to rule out the interference from dynamic objects, RANSAC (Random Sample Consensus) \cite{fischler1981random} is usually used. However, as the number of moving objects increases as the example shown in Fig. \ref{fig:features}(a), where among 785 feature points, 277 come from surrounding cars, it becomes extremely difficult in discriminating outliers and finding a reliable estimation to vehicle pose. Although Kundu \emph{et al}.\cite{kundu2011realtime}  implemented motion segmentation to remove the interference of surrounding moving objects in estimating ego-vehicle's pose, reliable motion segmentation in a high dynamic traffic scene is still an open problem.
Compared with point features, line features are dominant in a structured road environment. As shown in the column of Fig. 1 (b), the line segments in blue, red and green represent the road's longitude, latitude and vertical directions composing a local road frame.
From this set of line segments, three perpendicular axes relative to the road can be extracted. With such a feature, the surrounding cars are no longer interference factors, the line segments on environmental cars provide additional supports, which could be used to infer the rotation from ego-vehicle to a road frame.
So a new feature named road structural feature is defined as a set of line segments with direction of three perpendicular axes relative to the road together with feature points, which can be used for localization in a dynamic traffic environment.

\section{System Outline}
\label{sec_overview}
\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{source//Final//framework.pdf}
\caption{System framework. The key point is to define RSF and use it to calculate the vehicle pose.}
\label{fig:framework}
\end{figure}

In this research, a visual-based localization method is proposed by making use of the structural features on broad roads. The system flow is shown in Fig. \ref{fig:framework} consisting of three major steps, i.e. RSF prediction, RSF detection, and RSF-based vehicle pose estimation. They are detailed below.

\begin{figure}
  \centering
  \subfigure[Straight road situation]{
    \centering
    \includegraphics[width=0.9\linewidth]{source//Final//straight_road.pdf}
    \label{fig:straight_road}
  }
  \subfigure[Intersection situation]{
    \centering
    \includegraphics[width=0.9\linewidth]{source//Final//crossing.pdf}
    \label{fig:intersection}
  }
  \caption{Map-based RSF prediction. The road lines are given by the road map, but others like the building and the signboard, are manually added. The light blue trapezium is the camera's field of view and the top-left image is the RSF projection result on the image plane.}
  \label{fig:map_based_prediction}
\end{figure}

\subsubsection{Road structure prediction}
Now a day, many commercial maps are publicly available, such as the OpenStreetMap. With these maps, road lines at a predicted vehicle position are retrieved, and a road structure with three orthogonal axes $\tilde{RS}:\{\mathbf{e}_1,\mathbf{e}_2,\mathbf{e}_3\}$ is subsequently proposed as illustrated in Fig. \ref{fig:map_based_prediction} defining the longitudinal, lateral, and vertical directions of the road. The three orthogonal axes can be further projected onto the image frame according to the extrinsic and intrinsic calibration parameters of the camera, with which the line features are selected and discriminated in the next step. At an intersection scene, several $\tilde{RS}$s are predicted in similar ways, and the image frame is subsequently partitioned into several zones corresponding to each predicted $\tilde{RS}$ as illustrated in Fig. \ref{fig:intersection}.


\subsubsection{RSF detection}
In this step, a set of visual features $\mathbf{Z}_{RSF}:\{\mathbf{L}_1,\mathbf{L}_2,\mathbf{L}_3,\mathbf{P}\}$ consisting of selected line segments and points is developed, representing the  structural properties of the road. Line segments and feature points of general meaning are first detected from the current image frame using the methods in literature []. With a predicted $\tilde{RS}$, the line segments consistent with the predicted directions of $\mathbf{e}_1$, $\mathbf{e}_2$ and $\mathbf{e}_3$ are selected and classified accordingly into three groups, i.e. $\mathbf{L}_1$, $\mathbf{L}_2$ and $\mathbf{L}_3$, with a distance measure $\mathbf{d}$ that is defined in next section.
The line features on the lane markers and road boundaries along longitudinal road direction are consisted in $\mathbf{L}_1$. Lateral road markings and even the line segmented detected on environmental vehicles contribute to $\mathbf{L}_2$. $\mathbf{L}_3$ contains those from vertical objects, such as poles and buildings. On the other hand, the feature points that are tracked during two succeeding frames are selected in $\mathbf{P}$ at the present stage, while future work will make use of the end points of lane marker, poles by developing corresponding detection methods.


\subsubsection{RSF-based vehicle pose estimation}
A RSF candidate is defined as below, which can be used to estimate a vehicle pose $\mathbf{x}_i$ as detailed in next section.

\begin{equation}
\mathbf{C}_{RSF}=\{\mathbf{l}_1,\mathbf{l}_2\in \mathbf{L}_u, \mathbf{l}_3\in \mathbf{L}_v, u \neq v, \mathbf{p}_1,\mathbf{p}_2\in \mathbf{P}\}
\label{eq:C_RSF}
\end{equation}

Given a set of RSF features $\mathbf{Z}_{RSF}$, a number of candidates $\{\mathbf{C}_{RSF}^i\}$ can be enumerated according to the above definition. Fig. \ref{fig:features_for_estimation} illustrates a type of RSF candidate with $v=1$ and $u=3$, where $\mathbf{l}_1$ and $\mathbf{l}_2$ correspond to longitudinal road boundaries, and $\mathbf{l}_3$ is a vertical edge. Various types of candidates can be generated by changing the values of $u$ and $v$.

With the set of RSF features $\mathbf{Z}_{RSF}$ and candidates $\{\mathbf{C}_{RSF}^i\}$, the problem of vehicle pose estimation at frame $k$ is defined in a probabilistic way as below.

\begin{equation}
\hat{x_k} = \displaystyle \arg\max _{x_k^i \sim \mathbf{C}_{RSF}^i} \{ p(\mathbf{Z}_{RSF} | x_k^i) p(x_k^i|x_{k-1}) \}
\end{equation}

Each vehicle pose $\mathbf{x}_i$ estimated on a candidate $\{\mathbf{C}_{RSF}^i\}$ is evaluated, and the one that yields the maximal posterior of $p(\mathbf{Z}_{RSF} | x_k^i) p(x_k^i|x_{k-1})$ is selected. Here, $p(x_k^i|x_{k-1})$ is estimated in a Gaussian by assuming a linear vehicle motion model.

In the next section, we define a distance measure $\mathbf{d}$ in selecting and classifying the line segments with proposed road axes in RSF detection; detail the algorithm in estimating vehicle pose on a RSF candidate; and present an analytical estimation of the likelihood measure $p(\mathbf{Z}_{RSF} | x_k^i)$ in (2).



\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{source//Final//feature.pdf}
  \caption{The features for pose estimation. The line segments detected in a single frame are used for rotation estimation and the points detected and tracked in two frames are used for translation estimation.}
  \label{fig:features_for_estimation}
\end{figure}


\section{Algorithm Details}
\label{sec_details}

%Here the details of the algorithm is presented. The detection of RSF is given in the first place. After that a robust pose estimation algorithm based on RANSAC is explained, followed by the use of the map in RSF or RSFs prediction. Finally, some implementations are presented, which can higher the robustness of the system.

%\subsection{Notations and Coordinate Frames}
%Before going to the details, Some notations and coordinate frames are presented first. The road coordinate frame is made of the road's longitude, latitude and vertical directions, which are $X_r$, $Y_r$ and $Z_r$. For the vehicle frame, the origin is the center point of the rear axle, $X_v$ is to the right side, $Y_v$ is to the front and $Z_v$ is to the top. For the camera frame, the origin is the center point, $X_c$, $Y_c$ and $Z_c$ are to the left, down and front.
%
%$\mathbf{R}_r$ is the rotation of the road in world coordinate frame calculated by the angle $\theta$, which means
%\begin{equation}
%\mathbf{R}_r =\begin{bmatrix}   cos\theta & -sin\theta & 0 \\
%                                sin\theta & cos\theta & 0 \\
%                                0 & 0 & 1
%                    \end{bmatrix}
%\end{equation}
%So the road coordinate is
%\begin{equation}
%\begin{bmatrix} \mathbf{X}_r & \mathbf{Y}_r & \mathbf{Z}_r \end{bmatrix}
%= \mathbf{R}_r
%\begin{bmatrix} \mathbf{e}_1 & \mathbf{e}_2 & \mathbf{e}_3 \end{bmatrix}
%\end{equation}
%where $\mathbf{e}_1=(1,0,0)^T$, $\mathbf{e}_2=(0,1,0)^T$, $\mathbf{e}_3=(0,0,1)^T$ are the directions of x,y and z in world coordinate frame.
%
%$\mathbf{x}_{v} = \{\mathbf{R}_{v}, \mathbf{s}_{v}\}$ is the pose of the vehicle, where $\mathbf{R}_v$ is the orientation and $\mathbf{s}_v$ means the position. $\mathbf{x}_{c} = \{\mathbf{R}_{c}, \mathbf{s}_{c}\}$ is the pose of the camera. The coordinate transformation is provided by
%\begin{equation}
%\mathbf{R}_{c} = \mathbf{R}_v \mathbf{R}_{cv}
%\label{eq:R_c}
%\end{equation}
%\begin{equation}
%\mathbf{s}_{c} = \mathbf{s}_v + \mathbf{R}_{v} \mathbf{s}_{cv}
%\label{eq:S_c}
%\end{equation}
%where $\mathbf{R}_{cv}$ and $\mathbf{s}_{cv}$ are the orientation and position of the camera in vehicle coordinate frame, which is well calibrated.
%
%Let $\mathbf{P}=\mathbf{K}[\mathbf{R}|\mathbf{t}]$ is the projection matrix of the camera, where $\mathbf{K}$ is the intrinsic matrix, which is well calibrated, $\mathbf{R}=\mathbf{R}_c^T$ is the rotation matrix, and $\mathbf{t}=-\mathbf{R}_c\mathbf{s}_c$ is the translation vector.

\subsection{Pose Prediction and Coordinate Transformation}
Let $\mathbf{x}_{v} = \{\mathbf{R}_{v}, \mathbf{s}_{v}\}$ be a vehicle pose with respect to a world coordinate system, where $\mathbf{R}_{v}$ is a rotation matrix of the vehicle orientation, and $\mathbf{s}_{v}$ is a translation vector (i.e. vehicle position).
A vehicle frame is defined as a right-hand coordinate system with its origin at the center point of the rear axle, its $x$, $y$ and $z$ axes point to the right, front and up respectively. Let $\mathbf{e}_1=(1,0,0)^T$, $\mathbf{e}_2=(0,1,0)^T$, $\mathbf{e}_3=(0,0,1)^T$ be the three unit vectors, the vehicle frame at a world coordinate system can be represented as

\begin{equation}
\begin{bmatrix} \mathbf{d}_x & \mathbf{d}_y & \mathbf{d}_z \end{bmatrix}
= \mathbf{R}_v
\begin{bmatrix} \mathbf{e}_1 & \mathbf{e}_2 & \mathbf{e}_3 \end{bmatrix}
\end{equation}

Assuming a linear vehicle motion model, a vehicle pose $\hat{\mathbf{x}}_{v,k}=\{\hat{\mathbf{R}}_{v,k},\hat{\mathbf{s}}_{v,k}\}$ at frame $k$ can be predicted as below.

\begin{equation}
\hat{\mathbf{R}}_{v,k}=\mathbf{R}_{rel}\mathbf{R}_{v,k-1}=\left(\mathbf{R}_{v,k-1}\mathbf{R}_{v,k-2}^T\right)\mathbf{R}_{v,k-1}\\
\label{eq:R_t_predict}
\end{equation}
\begin{equation}
\hat{\mathbf{s}}_{v,k}= \mathbf{s}_{v,k-1} + \hat{\mathbf{d}}_{y,k} \int_{t_{k-1}}^{t_k}{v_t}\,dt
\end{equation}

where $\hat{\mathbf{d}}_{y,k}=\hat{\mathbf{R}}_{v,k}\mathbf{e}_2$ is a predicted vehicle's heading of frame $k$, $v$ is the speed detected from a wheel encoder.

A camera is rigidly mounted on the vehicle with its geometries well calibrated with the vehicle frame. Thus, a coordinate at the vehicle frame can be transformed onto the image plane with the following projection matrix.

\begin{equation}
\mathbf{P}_{vc}=\mathbf{K}[\mathbf{R}_{cv}^T|-\mathbf{R}_{cv}^T\mathbf{s}_{cv}]
\end{equation}

where $\mathbf{K}$ is the camera's intrinsic matrix, $\mathbf{R}_{cv}$ and $\mathbf{s}_{cv}$ are the rotation matrix and translation vector of the camera in vehicle coordinate frame.

Let $\mathbf{x}_{c} = \{\mathbf{R}_{c}, \mathbf{s}_{c}\}$ denote for a camera pose with respect to the world coordinate system. With the vehicle pose and calibration parameters, a camera pose at the frame $k$ is thus predicted as

\begin{equation}
\hat{\mathbf{R}}_{c,k} = \hat{\mathbf{R}}_{v,k} \mathbf{R}_{cv}
\label{eq:R_c}
\end{equation}
\begin{equation}
\hat{\mathbf{s}}_{c,k} = \hat{\mathbf{s}}_{v,k} + \hat{\mathbf{R}}_{v,k} \mathbf{s}_{cv}
\label{eq:S_c}
\end{equation}

Subsequently, a geometric primitive at the world coordinate system, e.g. a map object, can be transformed onto the image frame with the following projection matrix.

\begin{equation}
\mathbf{\hat{P}_{c}}=\mathbf{K}[\hat{\mathbf{R}}_{c,k}^T|-\hat{\mathbf{R}}_{c,k}^T\hat{\mathbf{s}}_{c,k}]
\end{equation}

\subsection{Map-based Road Structure Prediction}

Given a predicted vehicle and camera pose, road lines within the camera's vision field are retrieved from a map as illustrated in Fig. \ref{fig:map_based_prediction}. In case that the vehicle is on a straight road, one road line is obtained. At an intersection scene, a network of road lines are retrieved, with which the image plane is partitioned into several zones. For a curved road, the image plane is partitioned similarly, as a curved road can be represented using a sequence of straight road line segments. Below we describe the processing in each image zone, which corresponds to one road line.

Given a road line in a map, three orthogonal unit vectors, $\mathbf{d}_j, (j=1,2,3)$, are generated representing the road's longitudinal, lateral and vertical directions (briefly "road axis"). In case of a 2D road map, flat ground is assumed. In this paper, we take the matrix representation $\mathbf{R}_r = \begin{bmatrix} \mathbf{d}_1 & \mathbf{d}_2 & \mathbf{d}_3 \end{bmatrix}$, thus $\mathbf{d}_j = \mathbf{R}_r\mathbf{e}_j$ is the direction of each road axis in world coordinate system.

\subsection{Image Line Classification for RSF Detection}

After extracting line segments from the current image frame, classification is conducted to identify the image lines along each road axes in order for RSF candidates generation.
As a line $\mathbf{l}_i$ on image frame corresponds to a plane $\mathbf{\pi}_i^l = \left[\mathbf{l}_i^T\mathbf{K}, 0\right]^T$ at the camera frame, which has a normal of
\begin{equation}
\mathbf{n}_i=\frac{\mathbf{K}^T\mathbf{l}_i}{\left\|\mathbf{K}^T\mathbf{l}_i\right\|}
\end{equation}
a distance measure between an image line $\mathbf{l}_i$ and a road axis $\mathbf{R}_r\mathbf{e}_j$ is defined using the angle between $\mathbf{n}_i$ and $\mathbf{R}_r\mathbf{e}_j$, which are detailed at below.

\begin{equation}
d^2(\mathbf{l}_i,\mathbf{R}_r\mathbf{e}_j) = {\left(\mathbf{n}_i^T\mathbf{R}_c^T\mathbf{R}_r\mathbf{e}_j\right)}^2 = { \left(\frac  {\mathbf{l}_i^T\mathbf{K}\mathbf{R}_c^T\mathbf{R}_r\mathbf{e}_j}
                {\left\|\mathbf{K}^T\mathbf{l}_i\right\|}
    \right)
  }^2
\label{eq:d_3d}
\end{equation}

For any image line $\mathbf{l}_i$, a matched road axis is found if it has the shortest distance among three, and the distance is lower than a given threshold $\epsilon$, which compose the $\mathbf{L}_1$, $\mathbf{L}_2$, $\mathbf{L}_3$ of the RSF feautre $\mathbf{Z}_{RSF}$.

\begin{equation}
\hat{\mathbf{d}}_i = \mathbf{R}_r\mathbf{e}_{\hat{j}} \sim \hat{j} = \arg\min _{j} d^2(\mathbf{l}_i,\mathbf{R}_r\mathbf{e}_j)<\epsilon
\end{equation}

On the other hand, the image lines are filtered out if they are far from all road axes, i.e. the distance are larger than $\epsilon$.

\subsection{Pose Estimation using RSF Candidates}
As shown in (\ref{eq:C_RSF}), a RSF candidate is defined using three line segments and two points. Suppose the line segments are $\mathbf{l}_1,\mathbf{l}_2,\mathbf{l}_3$ with their classified directions $\hat{\mathbf{d}}_1,\hat{\mathbf{d}}_2,\hat{\mathbf{d}}_3$ ($\hat{\mathbf{d}}_1=\hat{\mathbf{d}}_2,\hat{\mathbf{d}}_1\cdot\hat{\mathbf{d}}_3=0$). $\mathbf{p}_1, \mathbf{p}_2$ are the feature points and $\mathbf{p}_1^\prime, \mathbf{p}_2^\prime$ are the tracked points in the previous frame. $\mathbf{P}=\mathbf{K}[\mathbf{R}|\mathbf{t}]$ is the projection matrix of the camera.
The line segments are used to estimate the rotation $\mathbf{R}$ and the points are for translation $\mathbf{t}$.
\subsubsection{Rotation Calculation}
As (\ref{eq:d_3d}) contains rotation parameter only, it is good for rotation calculation. As the degree of freedom (DOF) of the rotation matrix is three ($\mathbf{R}=SO(3)$), only three line segments and their directions are required.

Using the distance measure for line segments, three equations are given.
\begin{equation}
\mathbf{l}_1^T\mathbf{K}\mathbf{R}\hat{\mathbf{d}}_1=0, \mathbf{l}_2^T\mathbf{K}\mathbf{R}\hat{\mathbf{d}}_1=0
\label{eq:l_1,l_2}
\end{equation}
\begin{equation}
\mathbf{l}_3^T\mathbf{K}\mathbf{R}\hat{\mathbf{d}}_3=0
\label{eq:l_3}
\end{equation}

Based on (\ref{eq:l_1,l_2}), the projected vanishing point of $\hat{\mathbf{d}}_1$ is calculated as
\begin{equation}
\mathbf{v}_1 = \mathbf{l}_1\times \mathbf{l}_2 =  \lambda \mathbf{K}\mathbf{R}\hat{\mathbf{d}}_1
\label{eq:vp}
\end{equation}

Based on Rodriguez matrix, the rotation matrix can be written as
\begin{equation}
\mathbf{R}=\left(\mathbf{I}-\mathbf{S}\right)^{-1}\left(\mathbf{I}+\mathbf{S}\right)
\label{eq:Rodriguez_R}
\end{equation}
\begin{equation}
\mathbf{S}=\begin{bmatrix}      0 & -c & -b \\
                                c & 0 & -a \\
                                b & a & 0
                    \end{bmatrix}
\label{eq:Rodriguez_S}
\end{equation}

Substituting (\ref{eq:Rodriguez_R}) for (\ref{eq:vp}), the equation can be
\begin{equation}
(\mathbf{I}-\mathbf{S})\mathbf{K}^{-1}(\mathbf{l}_1\times \mathbf{l}_2)
= \lambda (\mathbf{I}+\mathbf{S})\hat{\mathbf{d}}_1
\label{eq:linear}
\end{equation}

Combining (\ref{eq:linear}) and (\ref{eq:l_3}), there are four parameters ($a,b,c,\lambda$) and four equations with three from (\ref{eq:linear}) and one from (\ref{eq:l_3}). So the parameters can be solved and there are up to two solutions. The explanation for the two solutions is that the direction $\hat{\mathbf{d}}_3$ could be either in front of the camera or behind the camera. This can be solved by matching with the predicted rotation.

\subsubsection{Translation Calculation}
Line segments are reliable on rotation, but so weak on translation. The reason is as follows. The line segments with direction of road's longitude are dominant, but they have no contribution in translation when the vehicle moves straight along the road. Those with direction of road's latitude contained many outliers detected from the moving vehicles. The vertical lines are reliable but so few in most cases. So two feature points is used to calculate the translation, as shown in the typical sample. Here an algorithm with epipolar geometry is used.

Suppose a feature point $\mathbf{p}$ is tracked from the previous frame $\mathbf{p}^\prime$. Let $\widetilde{\mathbf{X}}$ be the 3D point in world coordinate of $\mathbf{p}$, $\mathbf{X}$ and $\mathbf{X}^\prime$ be that in camera coordinate in the current frame and in previous frame. $\mathbf{P}=\mathbf{K}[\mathbf{R}|\mathbf{t}]$ is the projection matrix of the camera in the current frame, and $\mathbf{P}^\prime=\mathbf{K}[\mathbf{R}^\prime|\mathbf{t}^\prime]$ is that in the previous frame. The coordinate transformation equations are
\begin{equation}
\mathbf{X}    =\mathbf{R}       \widetilde{\mathbf{X}}+\mathbf{t}
\end{equation}
\begin{equation}
\mathbf{X}^\prime=\mathbf{R}^\prime   \widetilde{\mathbf{X}}+\mathbf{t}^\prime
\end{equation}

Then the relative transformation between two frames will be
\begin{equation}
\mathbf{X}=\mathbf{R}_{rel} \mathbf{X}^\prime + \mathbf{t}_{rel}
\label{eq:transformation_between_frames}
\end{equation}
where
\begin{equation}
\mathbf{R}_{rel} = \mathbf{R} {\mathbf{R}^\prime}^T
\end{equation}
\begin{equation}
\mathbf{t}_{rel}=\mathbf{t} - \mathbf{R}_{rel}^T \mathbf{t}^\prime
\end{equation}

Then three vectors are coplanar, which are $\mathbf{t}_{rel},\mathbf{X},\mathbf{X}-\mathbf{t}_{rel}$. So
\begin{equation}
  \mathbf{X}\cdot\left(\mathbf{t}_{rel}\times\left(\mathbf{X}-\mathbf{t}_{rel}\right)\right)=0
  \label{eq:coplanar_1}
\end{equation}
Substituting (\ref{eq:transformation_between_frames}) to (\ref{eq:coplanar_1}), we have
\begin{equation}
  \mathbf{X}_k\cdot\left(\mathbf{t}_{rel}\times\left(\mathbf{R}_{rel}\mathbf{X}^\prime\right)\right)=0
\end{equation}

Combined with the camera projection equation $\lambda \mathbf{p}=\mathbf{K}\mathbf{X}$, an equation with epipolar geometry constraint is given as
\begin{equation}
\mathbf{p}^T \mathbf{F} \mathbf{p}^\prime=0
\end{equation}
where
\begin{equation}
\mathbf{F} = \mathbf{K}^{-T} {\left[\mathbf{t}_{rel}\right]}_\times \mathbf{R}_{rel} \mathbf{K}^{-1}
\label{eq:F}
\end{equation}

As the rotation matrix is given, only two tracked points $\mathbf{p}_1,\mathbf{p}_2$ are required to calculate the relative translation $\mathbf{t}_{rel}$ using (\ref{eq:F}). The solution of $\mathbf{t}_{rel}$ is up to scale. The scale can be solved using the speed data of the vehicle provided by the encoder on the wheel.

\subsection{Likelihood measure}
To measure the likelihood $p(\mathbf{Z}_{RSF} | x_k^i)$ is to measure the observation errors for the features.
\begin{equation}
E= E_\mathbf{l} + \lambda E_\mathbf{p}
\end{equation}

For a line segment, it is provided by its length plus the distance between line segment and its direction.
\begin{equation}
E_\mathbf{l} = \sum_{i} \left(l_{i_1}^2+l_{i_2}^2\right) \cdot d^2(\mathbf{l}_i,\hat{\mathbf{d}}_i)
\end{equation}
where $l_{i_1},l_{i_2}$ are the first and second element of $\mathbf{l}_i$, and $l_{i_1}^2+l_{i_2}^2$ is the square of the length.

For a feature point, it is defined using the distance between the point and that projected by the matched point in the successive frame.
\begin{equation}
E_\mathbf{p} = \sum_{i} d^2(\mathbf{p}, \mathbf{p}^\prime)
\end{equation}
where
\begin{equation}
d^2(\mathbf{p}, \mathbf{p}^\prime)
= d^2 (\mathbf{p}, \mathbf{F} \mathbf{p}^\prime)
+ d^2 (\mathbf{p}^\prime, \mathbf{F}^T\mathbf{p}) \\
\end{equation}
Here $d^2 (\mathbf{p}, \mathbf{F} \mathbf{p}^\prime)$ is the square of distance from $p$ to the epipolar line $\mathbf{F} \mathbf{p}^\prime$.

\subsection{Robustness Concerns}
As addressed previously, in estimating a vehicle (or camera) pose, the typical sample of RSF should contains at least three line segments $\mathbf{l}_1,\mathbf{l}_2,\mathbf{l}_3$ with $\mathbf{d}_1=\mathbf{d}_2,\mathbf{d}_1\neq \mathbf{d}_3$, and two points tracked from the previous frame (see Fig. \ref{fig:features_for_estimation}).
However, in a complex dynamic environment, such a requirement might not always be met due to less types of dominant features, e.g. only the line segments on longitude directions are detected, or reliable point tracking on static object is not enough, etc. In order to improve the robustness of the approach for the frames with less number of features, simplified methods are developed using a prior knowledge of the vehicle motion model or horizontal vehicle motion assumption.

For the situation with few buildings or road lamps or vehicles, there are only line segments with road's longitude direction. In that case, the road could be assumed to be planar, so the rotation matrix $\mathbf{R}$ has only one degree of freedom and it can be solved using one equation $\mathbf{l}_1^T\mathbf{K}\mathbf{R}\mathbf{d}_1=0$.
%If we assume that the road is planar and the vehicle is moving smoothly without jolt, i.e. horizontal motion assumption, we can use only one line to compute the rotation. In this situation, the rotation matrix $\mathbf{R}$ has only one degree of freedom and it is easy to solve it using one equation $\mathbf{l}_1^T\mathbf{K}\mathbf{R}\mathbf{d}_1=0$.

%If the road is assumed to be planar, which is suitable in most cases, only vertical lines are required to calculate the translation. It is similar to the indoor line-based localization and useful in situations with a lot buildings or road lamps along the road. This algorithm can solve the curved road situation, since vertical lines are still stable in curved road.

For the heavy traffic situation, the feature points are not reliable since there are a lot of moving objects. Only the directions of line segments are stable. In that case, the rotation parameters is estimated by line segments, but the translation parameters are only predicted by the vehicle's motion model without estimation.
%If the vehicle motion model is reliable, it can be used to calculate the translation instead of points. It makes the algorithm more efficient and capable of dealing with the heavy traffic situation, as the feature points are not reliable in the situation with a lot moving objects.



\section{Experimental Results}
\label{sec_experiments}
Experiments have been conducted using the video streams collected on major roads in central Beijing. As shown in Fig. \ref{fig:scenario}, the roads are structured and dynamic. The video streams were collected using an instrumented vehicle with a monocular camera on its front roof, which has a resolution of $640\times480$ pixels at a frame rate of 15fps. As an on-line localization is concerned, the video streams were processed in this research at 3fps to balance computation cost. A calibration was conducted to find geometry between the camera and the vehicle, with which an image point can be projected to a ground plane by assuming it is flat. In addition, wheel speed of the vehicle was also recorded by an encoder, which is used in the motion model to predict a vehicle pose and for dealing with the scale problem of the translation vector. A high accurate GPS/IMU navigation system is used in data collection too, which acquire reference data for validation. Future work will be addressed on extending the approach to assist GPS localization. Below we present experimental results on a data set that was collected during a driving around 600 meters.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{source//Final//senario.pdf}
\caption{The scenario of the experiment. It takes two minutes driving about 600 meters with 350 frames taken from the camera on the roof of the vehicle. Green line is the trajectory got from the GPS/IMU system. Blue dash-line and red dash-dot-line denote for that of libviso2 and the proposed method. As the performance of GPS/IMU is not good at the end of the path, this part is ignored in validation.}
\label{fig:scenario}
\end{figure}

\begin{figure}[t]
\centering
    \includegraphics[width=1\linewidth]{source//Final//T_result.pdf}
\caption{The detail of positioning results in 2D space. The trajectories start at point (0,0) and green line, blue dash-line and red dash-dot-line represent those from GPS/IMU system, libviso2 and the proposed method.}
\label{fig:positions}
\end{figure}
\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{source//Final//R_result.pdf}
\caption{The yaw results. Green line, blue dash-line and red dash-dot-line represent those from GPS/IMU system, libviso2 and the proposed method.}
\label{fig:yaw}
\end{figure}

In this experiment, we use the LSD method \cite{von2012lsd} to detect line segments from a monocular image, and subsequently extract road structural feature (RSF), which is used to locate the vehicle pose with a road map. We compare the performance with the monocular part of libviso2 \cite{Geiger11}\cite{Geiger2013Visual}, which is a popular visual odometry method, and GPS/IMU localization data. Vehicle trajectories of the three methods are visualized on a Google Map (start at(39.9754760, 116.3911050)) as shown in Fig. \ref{fig:scenario}, where the red dash-dot-line, blue dash-line and green line denote for that of the proposed method, libviso2 and GPS/IMU outputs. In the experimental site, the sky is open with few tall buildings, thus the GPS/IMU data is quite accurate in most cases. However, the performance of the GPS/IMU is somewhat affected by the trees at the end of the path, as shown in the "END" area in Fig. \ref{fig:scenario}, which is ignored in validation. Due to the interferences from environmental cars and the high dynamics of ego-vehicle's motion during its turn, a big error happened in vehicle pose estimation using libviso2 and the trajectories diverse after the ego-vehicle finished right turn. On the other hand, the proposed algorithm demonstrate much better result comparing to that of libviso2, where the red dash-dot-line (i.e. result of the proposed method) and the green line have nearly perfect matching. One of the major reason for this is that the proposed method use a map to constrain vehicle pose estimation, which provides a prior knowledge of the road directions. More details in positioning results are shown in Fig. \ref{fig:positions}, where only 310 frames used in validation because of the bad performance of the GPS/IMU system at the end of the travel.

Estimations of the vehicle heading are plotted in Fig. \ref{fig:yaw}. It can be found out that the result of the proposed method (red dash-dot-line) is much closer to GPS/IMU data (green line), while it shows some zigzags, reflecting poor local consistencies. The main reason for this zigzags could be considered that inter-frame matching is not included in the proposed framework, which sacrifices local consistencies of the result.

\begin{table}[t]
\caption{The position and rotation error}
\centering
\begin{tabular}{|r||c||c|}
\hline
&Mean Position Error(m) & Mean Rotation Error(deg)\\
\hline
libviso2 & $5.6426$ & $3.346$\\
\hline
our method & $0.5609$ & $0.779$\\
\hline
\end{tabular}
\label{table:Error_Result}
\end{table}

The result of average error of position and heading is shown in Table. \ref{table:Error_Result}. The errors of libviso2 and the proposed method are calculated compared with GPS/IMU result in each frame. Then the results are calculated by averaging the absolute value of the errors. It shown that the proposed algorithm performs better than libviso2.

In order to visualize the results, a map containing road structural features can be subsequently generated as shown in Fig. \ref{fig:lineProjection}, where the line features with road's longitude direction are projected. By manual classification, lane markings, road curbs, zebra zones are represented. In that case, after overlapping ego-vehicle's trajectory on the map, the interactive behaviors, such as lane changing maneuvers, can be identified directly.

\begin{figure*}
\centering
%\includegraphics[width=1\linewidth]{source//fig9.png}
\includegraphics[width=1\linewidth]{source//Final//horizon_line.pdf}
\caption{The result of line segments projected onto horizontal plane. The line segments with road's longitude direction are projected. After manually marking the lines with lane markings, road curbs and zebra zones, a lane changing behavior can be identified.}
\label{fig:lineProjection}
\end{figure*}

Since the proposed method uses a 2D map, while libviso2 doesn't, it seems unfair to compare them. So another experiment has been done without the help of the map by assuming that our vehicle is driving in a straight road. It takes 4 minutes and 42 seconds driving about 3000 meters with 850 frames. The results are visualized on a Google Road Map (start at(39.9544800, 116.3031370)) as shown in Fig. \ref{fig:another_result}, where the red dash-dot-line, blue dash-line and green line denote for that of the proposed method, libviso2 and GPS/IMU outputs. The result of libviso2 is quit different from the GPS data after frame 412 because of the rotation error. Fig. \ref{fig:another_rotation_result} shows that on frame 419 and frame 619, the result of the yaw from libviso2 has two big jumps, which make the trajectory quit unusual. Since the road is non-strict straight, the result of our algorithm is a little different from the real trajectory after frame 400, where the direction of the road changes several degrees, as the GPS/IMU data shows in fig \ref{fig:another_rotation_result}.

\begin{figure}[t]
\centering
 \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{source//Final//T_result_in_map_Frame200_1049.pdf}
    \label{fig:another_T_in_map}
  \end{minipage}%
  \hfill
 \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{source//Final//T_result_Frame200_1049.pdf}
    \label{fig:another_T_result}
  \end{minipage}%\
\caption{The positioning result of the other experiment. It takes 4 minutes and 42 seconds driving about 3000 meters with 850 frames taken from the camera on the roof of the vehicle. Green line, blue dash-line and red dash-dot-line represent the trajectories from GPS/IMU system, libviso2 and the proposed method.}
\label{fig:another_result}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{source//Final//R_result_Frame200_1049.pdf}
\caption{The yaw result of the other experiment. Green line, blue dash-line and red dash-dot-line represent those from GPS/IMU system, libviso2 and the proposed method. The result of libviso2 shows two big errors on around frame 412 and frame 619.}
\label{fig:another_rotation_result}
\end{figure}

\section{Conclusions and Future Works}
\label{sec_conclusion}

In this research, a method using an on-board monocular camera is proposed to achieve precise localization at road lane level in structured road environments. In order to reduce the interference from dynamic traffics at the environment, so as to improve accuracy of a visual localization method, a road structural feature (RSF) is defined based on the line features with perpendicular directions and point features. The line features on such as lane markings, curbs, poles, building edges, etc., which are dominant, characterizes a structured road environment, and demonstrates the road's longitude, latitude and vertical directions.
A RSF-based visual localization method is proposed, where a prior knowledge to the environment, such as a 2D map with road-direction information, is implemented in a map-based localization framework, which predicts a RSF (or multiple RSFs at intersections) of the scene, and  estimates the vehicle pose after detecting the RSF from a monocular image with the predictions.
Experiments have been conducted using the video streams collected on major roads in central Beijing, which are structured and dynamic and a GPS/IMU data is recorded as a reference in validation.
The proposed algorithm shows better performance compared with a tradition visual odometry method.

While, the proposed algorithm have some limitations, which should be addressed in future work. As line segments are used to represent the features along three perpendicular axes of a structured road, the proposed algorithm works well on straight roads or turns. Extensions are needed for the scenes such as curved roads. On the other hand, the proposed work is utmost a position tracking algorithm, which requires a global localization to provide an initial position. It could be implemented as an assistant for GPS based localization, which will also be addressed in future work.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,IROS2013}

\end{document}
